{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 06 - CNNs for Handwritten Digits Recognition (\"MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This line configures matplotlib to show figures embedded in the notebook, \n",
    "# instead of opening a new window for each figure. More about that later. \n",
    "# If you are using an old version of IPython, try using '%pylab inline' instead.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group A\n",
    "In this exercise, you are asked to implement the so called LeNet-5 architecture, the first groundbreaking CNN architecture invented by Yan LeCun et al. [1], published in 1998. Implement the architecture in the method \"LeNet\" defined below. The function expects an input tensor and should output two things: The logits, and the softmax-activation of the logits. The entire code for training a CNN with tensorflow is provided, you simply have to plug in the \"LeNet\"-function at the right place in the code below, which is already done for you.\n",
    "\n",
    "Train a model using the provided training script and be prepared to interpret the training progress and results. A working set of default hyperparameters is already provided. \n",
    "\n",
    "[1] LeCun, Yann, et al. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE 86.11 (1998): 2278-2324."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet Architecture\n",
    "![LeNet Architecture](data/lenet.png)\n",
    "Source: Yan LeCun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LeNet(tensor):\n",
    "    # TODO\n",
    "\n",
    "    return logits, tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group B\n",
    "In this exercise, you are asked to implement a modified version of the so called AlexNet architecture, proposed by Alex Krizhevsky et al. in [2] for ImageNet. By default, it was intended to operate on 224x224x3px sized images from the ImageNet challenge. Implement the architecture in the method \"AlexNet\" defined below such that it can accept 28x28px sized input and output a 10-element vector with class probabilities (you have some creative freedom here). Therefore, leave out the first block consisting of convolution, local response normalization and max-pooling (refer to the paper or online resources for more details on the architecture). For the fully connected layers, use two consecutive layers with 120 and 84 neurons each instead of 4096 (as seen in LeNet in Exercise for Group A). The function expects an input tensor and should output two things: The logits, and the softmax-activation of the logits. The entire code for training a CNN with tensorflow is provided, you simply have to plug in the \"AlexNet\"-function at the right place in the code below (substitute the call of the LeNet-function with AlexNet).\n",
    "\n",
    "Be prepared to answer the following questions: \n",
    "* How does FC8 have to be modified such that it works for MNIST?\n",
    "* How does Layer Response Normalization work?\n",
    "* If you don't have a GPU: does it train on your CPU? How many parameters does it have compared to LeNet (estimate)?\n",
    "\n",
    "[2] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012.\n",
    "\n",
    "![AlexNet architecture](data/alexnet.png)\n",
    "Source: https://www.saagie.com/blog/object-detection-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AlexNet(tensor):    \n",
    "    #TODO\n",
    "    \n",
    "    return logits, tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group C\n",
    "In this exercise, you are asked to implement a modified version of the so called VGG-16 network architecture, originally proposed by Simonyan et al. [3] in 2014 for classifying 224x224x3 sized images into 1000 classes on behalf of the ImageNet challenge. The architecture by default is too deep for MNIST. Be creative and implement a shallower variant of the architecture in the method \"VGGNet\" defined below such that it can accept 28x28px sized input and outputs a 10-element vector with class probabilities (Hint: you can for instance leave out columns 5-10 in the table describing the architecture, and use half the number of filters). Make sure you do not change the convolution kernel size of 3x3 which is very characteristic for VGG. For the fully connected layers, use two consecutive layers with 120 and 84 neurons each instead of 4096 (as seen in LeNet in Exercise for Group A). The function expects an input tensor and should output two things: The logits, and the softmax-activation of the logits. The entire code for training a CNN with tensorflow is provided, you simply have to plug in the \"VGGNet\"-function at the right place in the code below (substitute the call of the LeNet-function with VGGNet).\n",
    "\n",
    "Be prepared to answer the following questions (It is recommended to refer to the paper for more details):\n",
    "- What is it that makes the VGG-Net architecture so special?\n",
    "- Does it make sense to make VGG-Net so shallow for MNIST?\n",
    "\n",
    "[3] Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognition.\" arXiv preprint arXiv:1409.1556 (2014).\n",
    "\n",
    "![VGG16 architecture](data/vgg16.png)\n",
    "Source: http://mmlab.ie.cuhk.edu.hk/projects/PCN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VGGNet(tensor):\n",
    "    #TODO\n",
    "    \n",
    "    return logits, tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group D\n",
    "In this exercise, you are asked to implement a ResNet-like architecture, as proposed by He et al. [4] in 2016, originally proposed for the ImageNet challenge. Implement the architecture described below in the method \"ResNet\" such that it can accept 28x28px sized input and output a 10-element vector with class probabilities. Use only 2 consecutive residual blocks before ending in the fully connected layers. For the fully connected layers, use two consecutive layers with 120 and 84 neurons each (as seen in LeNet in Exercise for Group A). The function expects an input tensor and should output two things: The logits, and the sigmoid-activation of the logits. The entire code for training a CNN with tensorflow is provided, you simply have to plug in the \"ResNet\"-function at the right place in the code below (substitute the LeNet-function with the ResNet-function).\n",
    "\n",
    "Be prepared to explain the concept of residual blocks!\n",
    "\n",
    "Architecture: \n",
    "* Residual Block of Depth 1 with 32 Filters, 3x3 Kernels and ReLu activations\n",
    "* MaxPooling with Kernelsize 2x2 and stride 2\n",
    "* Residual Block of Depth 1 with 64 Filters, 3x3 Kernels and ReLu activations\n",
    "* MaxPooling with Kernelsize 2x2 and stride 2\n",
    "* Fully connected layer with 120 Neurons\n",
    "* ReLu Activation\n",
    "* Fully connected layer with 84 Neurons\n",
    "* ReLu Activation\n",
    "* Output Fully connected layer\n",
    "\n",
    "Below is a depiction of a residual block:\n",
    "![VGG16 architecture](data/residualblock.png)\n",
    "\n",
    "[4] He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#RESNET\n",
    "#Helper Functions\n",
    "def weights_init(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def bias_init(shape, bias_value=0.01):\n",
    "    return tf.Variable(tf.constant(bias_value, shape=shape))\n",
    "\n",
    "def conv2d_custom(input, filter_size, num_of_channels, num_of_filters, activation=tf.nn.relu, dropout=None,\n",
    "                  padding='SAME', max_pool=True, strides=(1, 1)):  \n",
    "    weights = weights_init([filter_size, filter_size, num_of_channels, num_of_filters])\n",
    "    bias = bias_init([num_of_filters])\n",
    "    \n",
    "    layer = tf.nn.conv2d(input, filter=weights, strides=[1, strides[0], strides[1], 1], padding=padding) + bias\n",
    "    \n",
    "    if activation != None:\n",
    "        layer = activation(layer)\n",
    "    \n",
    "    if max_pool:\n",
    "        layer = tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    if dropout != None:\n",
    "        layer = tf.nn.dropout(layer, dropout)\n",
    "        \n",
    "    return layer\n",
    "def flatten(layer):\n",
    "   \n",
    "    shape = layer.get_shape()\n",
    "    \n",
    "    num_elements_ = shape[1:4].num_elements()\n",
    "    \n",
    "    flattened_layer = tf.reshape(layer, [-1, num_elements_])\n",
    "    return flattened_layer, num_elements_\n",
    "def dense_custom(input, input_size, output_size, activation=tf.nn.relu, dropout=None):\n",
    "   \n",
    "    weights = weights_init([input_size, output_size])\n",
    "    bias = bias_init([output_size])\n",
    "    \n",
    "    layer = tf.matmul(input, weights) + bias\n",
    "    \n",
    "    if activation != None:\n",
    "        layer = activation(layer)\n",
    "    \n",
    "    if dropout != None:\n",
    "        layer = tf.nn.dropout(layer, dropout)\n",
    "        \n",
    "    return layer\n",
    "def residual_unit(layer):\n",
    "    step1 = tf.layers.batch_normalization(layer)\n",
    "    step2 = tf.nn.relu(step1)\n",
    "    step3 = conv2d_custom(step2, 3, 32, 32, activation=None, max_pool=False) #32 number of feautres is hyperparam\n",
    "    step4 = tf.layers.batch_normalization(step3)\n",
    "    step5 = tf.nn.relu(step4)\n",
    "    step6 = conv2d_custom(step5, 3, 32, 32, activation=None, max_pool=False)\n",
    "    return layer + step6\n",
    "num_of_layers = 20\n",
    "between_strides = num_of_layers/5\n",
    "\n",
    "def ResNet(tensor):\n",
    "    inputs=tensor \n",
    "    prev1 = conv2d_custom(inputs, 3, 1, 32, activation=None, max_pool=False)\n",
    "    prev1 = tf.layers.batch_normalization(prev1)\n",
    "    for i in range(3): # this number * between_strides = number_of_layers\n",
    "        for j in range(int(between_strides)):\n",
    "            prev1 = residual_unit(prev1)\n",
    "    #After 2 res units we perform strides 2x2, which will reduce data\n",
    "    prev1 = conv2d_custom(inputs, 3, 1, 32, activation=None, max_pool=False, strides=[2, 2])\n",
    "    prev1 = tf.layers.batch_normalization(prev1)\n",
    "    #after all resunits we have last conv layer, than flattening and output layer\n",
    "    last_conv = conv2d_custom(prev1, 3, 32, 10, activation=None, max_pool=False)\n",
    "    flat, features = flatten(last_conv)\n",
    "    logits = dense_custom(flat, features, 10, activation=None)\n",
    "    return logits, tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training code starts here ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualizeCurves(curves, handle=None):\n",
    "    if not handle:\n",
    "        handle = plt.figure()\n",
    "\n",
    "    fig = plt.figure(handle.number)\n",
    "    fig.clear()\n",
    "    ax = plt.axes()\n",
    "    plt.cla()\n",
    "\n",
    "    counter = len(curves[list(curves.keys())[0]])\n",
    "    x = np.linspace(0, counter, num=counter)\n",
    "    for key, value in curves.items():\n",
    "        value_ = np.array(value).astype(np.double)\n",
    "        mask = np.isfinite(value_)\n",
    "        ax.plot(x[mask], value_[mask], label=key)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"Curves\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "    \n",
    "def printNumberOfTrainableParams():\n",
    "    total_parameters = 0\n",
    "    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    for variable in variables:\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        # print(shape)\n",
    "        # print(len(shape))\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            #    print(dim)\n",
    "            variable_parametes *= dim.value\n",
    "        # print(variable_parametes)\n",
    "        total_parameters += variable_parametes\n",
    "    print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "config = {}\n",
    "config['batchsize'] = 128\n",
    "config['learningrate'] = 0.01\n",
    "config['numEpochs'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Download and read in MNIST automatically\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print(\"Image Shape: {}\".format(mnist.train.images[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(mnist.train.images)))\n",
    "print(\"Validation Set: {} samples\".format(len(mnist.validation.images)))\n",
    "print(\"Test Set:       {} samples\".format(len(mnist.test.images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fae98d23320>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize a sample from MNIST\n",
    "index = random.randint(0, len(mnist.train.images))\n",
    "image = mnist.train.images[index].reshape((28, 28))\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252756\n"
     ]
    }
   ],
   "source": [
    "# Clear Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define placeholders\n",
    "inputs = {}\n",
    "inputs['data'] = tf.placeholder(tf.float32, [None, 32, 32, 1])\n",
    "inputs['labels'] = tf.placeholder(tf.float32, [None, 10])\n",
    "inputs['phase'] = tf.placeholder(tf.bool)\n",
    "\n",
    "# Define a dictionary for storing curves\n",
    "curves = {}\n",
    "curves['training'] = []\n",
    "curves['validation'] = []\n",
    "\n",
    "# Instantiate the model operations\n",
    "logits, probabilities = ResNet(inputs['data']) # Or VGGNet or ResNet or AlexNet\n",
    "printNumberOfTrainableParams()\n",
    "\n",
    "# Define loss function in a numerically stable way\n",
    "# DONT: cross_entropy = tf.reduce_mean(-tf.reduce_sum( * tf.log(y), reduction_indices=[1]))\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = inputs['labels']))\n",
    "\n",
    "# Operations for assessing the accuracy of the classifier\n",
    "correct_prediction = tf.equal(tf.argmax(probabilities,1), tf.argmax(inputs['labels'],1))\n",
    "accuracy_operation = tf.cast(correct_prediction, tf.float32)\n",
    "\n",
    "# Idea: Use different optimizers?\n",
    "# SGD vs ADAM\n",
    "#train_step = tf.train.AdamOptimizer(config['learningrate']).minimize(cross_entropy)\n",
    "train_step = tf.train.GradientDescentOptimizer(config['learningrate']).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create TensorFlow Session and initialize all weights\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkpL/C8+sxp1cawc1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae92595ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run!\n",
    "numTrainSamples = len(mnist.train.images)\n",
    "numValSamples = len(mnist.validation.images)\n",
    "numTestSamples = len(mnist.test.images)\n",
    "for e in range(config['numEpochs']):\n",
    "    avg_loss_in_current_epoch = 0\n",
    "    for i in range(0, numTrainSamples, config['batchsize']):\n",
    "        batch_data, batch_labels = mnist.train.next_batch(config['batchsize'])\n",
    "        batch_data = batch_data.reshape((batch_data.shape[0], 28, 28, 1))\n",
    "        \n",
    "        batch_data = np.pad(batch_data, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "        fetches = {\n",
    "            'optimizer': train_step,\n",
    "            'loss': cross_entropy\n",
    "        }\n",
    "        results = sess.run(fetches, feed_dict={inputs['data']: batch_data, inputs['labels']: batch_labels})\n",
    "        avg_loss_in_current_epoch += results['loss']\n",
    "    avg_loss_in_current_epoch = avg_loss_in_current_epoch / i\n",
    "    curves['training'] += [avg_loss_in_current_epoch]\n",
    "        \n",
    "    for i in range(0, numValSamples, config['batchsize']):\n",
    "        # Use Matplotlib to visualize the loss on the training and validation set\n",
    "        batch_data, batch_labels = mnist.validation.next_batch(config['batchsize'])\n",
    "        batch_data = batch_data.reshape((batch_data.shape[0], 28, 28, 1))\n",
    "        \n",
    "        # TODO: Preprocess the images in the batch\n",
    "        batch_data = np.pad(batch_data, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "        \n",
    "        fetches = {\n",
    "            'loss': cross_entropy\n",
    "        }\n",
    "        results = sess.run(fetches, feed_dict={inputs['data']: batch_data, inputs['labels']: batch_labels})\n",
    "        avg_loss_in_current_epoch += results['loss']\n",
    "    avg_loss_in_current_epoch = avg_loss_in_current_epoch / i\n",
    "    curves['validation'] += [avg_loss_in_current_epoch]\n",
    "    \n",
    "    print('Done with epoch %d' % (e))\n",
    "    visualizeCurves(curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.920703\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "accumulated_predictions = np.array([])\n",
    "for i in range(0, numValSamples, config['batchsize']):\n",
    "    # Use Matplotlib to visualize the loss on the training and validation set\n",
    "    batch_data, batch_labels = mnist.test.next_batch(config['batchsize'])\n",
    "    batch_data = batch_data.reshape((batch_data.shape[0], 28, 28, 1))\n",
    "        \n",
    "    # TODO: Preprocess the images in the batch\n",
    "    batch_data = np.pad(batch_data, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    fetches = {\n",
    "        'accuracy': accuracy_operation\n",
    "    }\n",
    "    results = sess.run(fetches, feed_dict={inputs['data']: batch_data, inputs['labels']: batch_labels})\n",
    "    \n",
    "    if i==0:\n",
    "        accumulated_predictions = results['accuracy']\n",
    "    else:\n",
    "        accumulated_predictions = np.append(accumulated_predictions, results['accuracy'])\n",
    "accuracy = np.mean(accumulated_predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
